{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "LSTM - Part 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9dZX_eBI-V7",
        "colab_type": "text"
      },
      "source": [
        "## Assignment : 14"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-MfOXBCI-WC",
        "colab_type": "text"
      },
      "source": [
        "<pre>\n",
        "1. Preprocess all the Data we have in DonorsChoose <a href='https://drive.google.com/drive/folders/1MIwK7BQMev8f5CbDDVNLPaFGB32pFN60'>Dataset</a> use train.csv\n",
        "2. Combine 4 essay's into one column named - 'preprocessed_essays'. \n",
        "3. After step 2 you have to train 3 types of models as discussed below. \n",
        "4. For all the model use <a href='https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics'>'auc'</a> as a metric. check <a href='https://datascience.stackexchange.com/a/20192'>this</a> for using auc as a metric \n",
        "5. You are free to choose any number of layers/hiddden units but you have to use same type of architectures shown below. \n",
        "6. You can use any one of the optimizers and choice of Learning rate and momentum, resources: <a href='http://cs231n.github.io/neural-networks-3/'>cs231n class notes</a>, <a href='https://www.youtube.com/watch?v=hd_KFJ5ktUc'>cs231n class video</a>. \n",
        "7. For all the model's use <a href='https://www.youtube.com/watch?v=2U6Jl7oqRkM'>TensorBoard</a> and plot the Metric value and Loss with epoch. While submitting, take a screenshot of plots and include those images in .ipynb notebook and PDF. \n",
        "8. Use Categorical Cross Entropy as Loss to minimize.\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrKPKrTNI-WG",
        "colab_type": "text"
      },
      "source": [
        "### Model-1\n",
        "\n",
        "Build and Train deep neural network as shown below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wokkQY_nI-WK",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://i.imgur.com/w395Yk9.png'>\n",
        "ref: https://i.imgur.com/w395Yk9.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD51j3-NI-WN",
        "colab_type": "text"
      },
      "source": [
        "- __Input_seq_total_text_data__ --- You have to give Total text data columns. After this use the Embedding layer to get word vectors. Use given predefined glove word vectors, don't train any word vectors. After this use LSTM and get the LSTM output and Flatten that output. \n",
        "- __Input_school_state__ --- Give 'school_state' column as input to embedding layer and Train the Keras Embedding layer. \n",
        "- __Project_grade_category__  --- Give 'project_grade_category' column as input to embedding layer and Train the Keras Embedding layer.\n",
        "- __Input_clean_categories__ --- Give 'input_clean_categories' column as input to embedding layer and Train the Keras Embedding layer.\n",
        "- __Input_clean_subcategories__ --- Give 'input_clean_subcategories' column as input to embedding layer and Train the Keras Embedding layer.\n",
        "- __Input_clean_subcategories__ --- Give 'input_teacher_prefix' column as input to embedding layer and Train the Keras Embedding layer.\n",
        "- __Input_remaining_teacher_number_of_previously_posted_projects._resource_summary_contains_numerical_digits._price._quantity__ ---concatenate remaining columns and add a Dense layer after that. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIVeesAYI-WR",
        "colab_type": "text"
      },
      "source": [
        "- For LSTM, you can choose your sequence padding methods on your own or you can train your LSTM without padding, there is no restriction on that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuKnzS93I-WU",
        "colab_type": "text"
      },
      "source": [
        "Below is an example of embedding layer for a categorical columns. In below code all are dummy values, we gave only for referance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE4JzSogI-WY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work\n",
        "input_layer = Input(shape=(n,))\n",
        "embedding = Embedding(no_1, no_2, input_length=n)(input_layer)\n",
        "flatten = Flatten()(embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVQpcCHvJClP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "f372ddc0-7d9e-4a7f-9086-346b769ae807"
      },
      "source": [
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from numpy import zeros\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Input\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM,Bidirectional\n",
        "from keras.layers.core import Dense, Dropout\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "import keras\n",
        "from tensorboardcolab import *\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import LeakyReLU\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3VX8k9JJGPa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "3988faf1-7684-41e9-d036-7ef5230cc2be"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RVQhTyhJgAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/Applied ML assignments/preprocessed_data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMOcOH6DJosb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "5f318d8c-0ce9-4f70-8883-077097872dc6"
      },
      "source": [
        "resource_data = pd.read_csv('/content/drive/My Drive/LSTM Assignment/resources.csv')\n",
        "resource_data.columns\n",
        "project_data = pd.read_csv('/content/drive/My Drive/LSTM Assignment/train_data.csv')\n",
        "project_data.columns"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'id', 'teacher_id', 'teacher_prefix', 'school_state',\n",
              "       'project_submitted_datetime', 'project_grade_category',\n",
              "       'project_subject_categories', 'project_subject_subcategories',\n",
              "       'project_title', 'project_essay_1', 'project_essay_2',\n",
              "       'project_essay_3', 'project_essay_4', 'project_resource_summary',\n",
              "       'teacher_number_of_previously_posted_projects', 'project_is_approved'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZaZJAY9Jru1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "def3adcf-bd29-4f10-f628-fb2d22e10240"
      },
      "source": [
        "price_data = resource_data.groupby('id').agg({'price':'sum', 'quantity':'sum'}).reset_index()\n",
        "price_data.head(2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>price</th>\n",
              "      <th>quantity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>p000001</td>\n",
              "      <td>459.56</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>p000002</td>\n",
              "      <td>515.89</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id   price  quantity\n",
              "0  p000001  459.56         7\n",
              "1  p000002  515.89        21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TwZ9VCQJxC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "project_data = pd.merge(project_data, price_data, on='id', how='left')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28685wqXJ1Bo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e866c084-badc-4a5d-e616-ad39dc9686a1"
      },
      "source": [
        "project_data['quantity'].shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(109248,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRjPCaVKKLbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['quantity'] = project_data['quantity']\n",
        "#df1['columename']= df2['existing_colume_name']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2F2K80EKPRc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "785e4461-d021-4794-a64f-5bbb9f31b8fa"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['school_state', 'teacher_prefix', 'project_grade_category',\n",
              "       'teacher_number_of_previously_posted_projects', 'project_is_approved',\n",
              "       'clean_categories', 'clean_subcategories', 'essay', 'price',\n",
              "       'quantity'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8bQhq_uKSvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#assigning class labels\n",
        "y=df['project_is_approved']\n",
        "df.drop(['project_is_approved'],axis=1, inplace=True)\n",
        "x=df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILT4Afj8abKg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "f70e6c29-4866-4326-874a-f94150be4642"
      },
      "source": [
        "#x.columns\n",
        "#x.drop(['teacher_number_of_previously_posted_projects','price','quantity'],axis=1, inplace=True)\n",
        "x.columns\n",
        "#col = ['teacher_prefix', 'school_state', 'project_grade_category',\n",
        "       #'clean_categories', 'clean_subcategories','essay',\n",
        "       #'remaining_input']\n",
        "#x = x[col]\n",
        "#x.columns"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['school_state', 'teacher_prefix', 'project_grade_category',\n",
              "       'teacher_number_of_previously_posted_projects', 'clean_categories',\n",
              "       'clean_subcategories', 'essay', 'price', 'quantity'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anbbhUWTKVma",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "ad474b2a-053d-412a-a987-f47c2babe811"
      },
      "source": [
        "#Splitting into train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "#Splitting train data into train and cv(60:20)\n",
        "X_tr, X_cv, y_tr, y_cv = train_test_split(X_train, y_train, test_size=0.2)\n",
        "print(X_tr.shape, y_tr.shape)\n",
        "print(X_cv.shape, y_cv.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(87398, 9) (87398,)\n",
            "(21850, 9) (21850,)\n",
            "(69918, 9) (69918,)\n",
            "(17480, 9) (17480,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qThFW0T4KoA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://stackoverflow.com/questions/21057621/sklearn-labelencoder-with-never-seen-before-values\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "class LabelEncoderExt(object):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
        "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
        "        \"\"\"\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        # self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "    def fit(self, data_list):\n",
        "        \"\"\"\n",
        "        This will fit the encoder for all the unique values and introduce unknown value\n",
        "        :param data_list: A list of string\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
        "        self.classes_ = self.label_encoder.classes_\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_list):\n",
        "        \"\"\"\n",
        "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
        "        :param data_list:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        new_data_list = list(data_list)\n",
        "        for unique_item in np.unique(data_list):\n",
        "            if unique_item not in self.label_encoder.classes_:\n",
        "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
        "\n",
        "        return self.label_encoder.transform(new_data_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3GQs-tqKYzi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "f812ea26-cb7a-45e6-910c-1e89e5d50c16"
      },
      "source": [
        "#teacher_prefix\n",
        "vectorizer = LabelEncoderExt()\n",
        "vectorizer.fit(X_tr['teacher_prefix'].values)\n",
        "teacherprefix_ohe_train = vectorizer.transform(X_tr['teacher_prefix'].values)\n",
        "teacherprefix_ohe_cv = vectorizer.transform(X_cv['teacher_prefix'].values)\n",
        "teacherprefix_ohe_test = vectorizer.transform(X_test['teacher_prefix'].values)\n",
        "print(teacherprefix_ohe_cv.shape)\n",
        "print(teacherprefix_ohe_train.shape)\n",
        "print(teacherprefix_ohe_test.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17480,)\n",
            "(69918,)\n",
            "(21850,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSy6DTRwKtRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Converting categorical features to One hot encoded features\n",
        "#clean_categories\n",
        "vectorizer = LabelEncoderExt()\n",
        "vectorizer.fit(X_tr['clean_categories'].values)\n",
        "categories_one_hot_train = vectorizer.transform(X_tr['clean_categories'].values)\n",
        "categories_one_hot_cv = vectorizer.transform(X_cv['clean_categories'].values)\n",
        "categories_one_hot_test = vectorizer.transform(X_test['clean_categories'].values)\n",
        "\n",
        "#clean_subcategories\n",
        "vectorizer = LabelEncoderExt()\n",
        "vectorizer.fit(X_tr['clean_subcategories'].values)\n",
        "subcategories_one_hot_train = vectorizer.transform(X_tr['clean_subcategories'].values)\n",
        "subcategories_one_hot_cv = vectorizer.transform(X_cv['clean_subcategories'].values)\n",
        "subcategories_one_hot_test = vectorizer.transform(X_test['clean_subcategories'].values)\n",
        "\n",
        "#school_state\n",
        "vectorizer = LabelEncoderExt()\n",
        "vectorizer.fit(X_tr['school_state'].values)\n",
        "schoolstate_one_hot_train = vectorizer.transform(X_tr['school_state'].values)\n",
        "schoolstate_one_hot_cv = vectorizer.transform(X_cv['school_state'].values)\n",
        "schoolstate_one_hot_test = vectorizer.transform(X_test['school_state'].values)\n",
        "\n",
        "\n",
        "#project_grade_category\n",
        "vectorizer = LabelEncoderExt()\n",
        "vectorizer.fit(X_tr['project_grade_category'].values)\n",
        "project_grade_category_one_hot_train = vectorizer.transform(X_tr['project_grade_category'].values)\n",
        "project_grade_category_one_hot_cv = vectorizer.transform(X_cv['project_grade_category'].values)\n",
        "project_grade_category_one_hot_test = vectorizer.transform(X_test['project_grade_category'].values)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3q-Y440KxHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Concatenating numerical features\n",
        "rem_input_train = np.concatenate((X_tr['quantity'].values.reshape(-1,1),X_tr['price'].values.reshape(-1,1),X_tr['teacher_number_of_previously_posted_projects'].values.reshape(-1,1)), axis=1)\n",
        "rem_input_cv = np.concatenate((X_cv['quantity'].values.reshape(-1,1),X_cv['price'].values.reshape(-1,1),X_cv['teacher_number_of_previously_posted_projects'].values.reshape(-1,1)), axis=1)\n",
        "rem_input_test = np.concatenate((X_test['quantity'].values.reshape(-1,1),X_test['price'].values.reshape(-1,1),X_test['teacher_number_of_previously_posted_projects'].values.reshape(-1,1)), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrhUKp_pK0uV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "423d7938-7e96-4ac9-9d85-3787d2960bd9"
      },
      "source": [
        "y_train = to_categorical(y_tr)\n",
        "y_cv = to_categorical(y_cv)\n",
        "y_test = to_categorical(y_test)\n",
        "y_test.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21850, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGm11bBJ_geI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_names[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dApzbgWucjLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_names[5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T7UD5jPK258",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "05fc2c16-7f90-4554-e8dd-9ba33c9c0c75"
      },
      "source": [
        "#Integer encoding Essay column using tokenizer API\n",
        "#https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(X_tr['essay'])\n",
        "vocab_size = len(t.word_index) + 1\n",
        "print(\"Vocabulary size_train:\",vocab_size)\n",
        "max_length = 400\n",
        "# integer encode the train data\n",
        "encoded_docs = t.texts_to_sequences(X_tr['essay'])\n",
        "essay_pad_train = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(essay_pad_train.shape)\n",
        "\n",
        "\n",
        "# integer encode the cv data\n",
        "encoded_docs = t.texts_to_sequences(X_cv['essay'])\n",
        "essay_pad_cv = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(essay_pad_cv.shape)\n",
        "\n",
        "\n",
        "# integer encode the test data\n",
        "encoded_docs = t.texts_to_sequences(X_test['essay'])\n",
        "essay_pad_test = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(essay_pad_test.shape)\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size_train: 47307\n",
            "(69918, 400)\n",
            "(17480, 400)\n",
            "(21850, 400)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5WLFAaYLa6b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6d0306f3-9574-4381-e9a5-365b2d66d5e0"
      },
      "source": [
        "#Embedding using Glove vectors\n",
        "embeddings_index = dict()\n",
        "f = open(r'/content/drive/My Drive/Applied ML assignments/glove.6B.300d.txt')\n",
        "#with open('/content/drive/My Drive/Applied ML assignments/glove_vectors', 'rb') as f:\n",
        "  #text = f.read()\n",
        "for line in f:\n",
        "  #line.decode(errors='ignore')\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsLj4qVbLe5U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1fab5c87-1194-4dd8-90c3-42b9e7129edd"
      },
      "source": [
        "#create a weight matrix for words in training docs\n",
        "embedding_matrix = zeros((vocab_size, 300))\n",
        "for word, i in t.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector\n",
        "\n",
        "print(\"embedding matrix shape\",embedding_matrix.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedding matrix shape (47307, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJsEhznxLhTJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "eaf9f86b-e466-4c67-95cf-32fed621d2df"
      },
      "source": [
        "#Flattening the text input data after calculating embedding matrix using glove vectors\n",
        "ins = []\n",
        "concat = []\n",
        "text_input = Input(shape=(max_length,), name = \"text_input\")\n",
        "# max_length = 400 ---->max length of sentence\n",
        "ins.append(text_input)\n",
        "e1 = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length,trainable=False)(text_input)\n",
        "\n",
        "l1= LSTM(128,kernel_regularizer=l2(0.001),return_sequences=True)(e1)\n",
        "#l1= LeakyReLU(alpha = 0.3)(l1)\n",
        "f1= Flatten()(l1)\n",
        "concat.append(f1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmAFZM0HLvaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Combining Categorical features\n",
        "#https://medium.com/@davidheffernan_99410/an-introduction-to-using-categorical-embeddings-ee686ed7e7f9\n",
        "\n",
        "cat_vars = [\"teacher_prefix\",\"school_state\",\"project_grade_category\",\"clean_categories\",\"clean_subcategories\"]\n",
        "\n",
        "cat_sizes = {}\n",
        "cat_embsizes = {}\n",
        "for cat in cat_vars:\n",
        "    cat_sizes[cat] = X_tr[cat].nunique()#nunique - includes unique elements\n",
        "    cat_embsizes[cat] = min(50, cat_sizes[cat]//2+1)#embedding size is chosen as half the size of unique elements + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3gMmW4ALy2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now we iterate over our categorical variables and create an input layer → embedding layer → reshape layer\n",
        "for cat in cat_vars:\n",
        "    x = Input((1,), name=cat)\n",
        "    ins.append(x)\n",
        "    x = Embedding(cat_sizes[cat]+1, cat_embsizes[cat], input_length=1)(x)\n",
        "    x = Flatten()(x)\n",
        "    concat.append(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScbTHSYAL1c1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Converting the remaining input using Dense layer\n",
        "rem_input_layer =  Input(shape=(3,), name=\"rem_input_layer\")\n",
        "ins.append(rem_input_layer)\n",
        "rem_input_dense = Dense(64, activation='relu')(rem_input_layer)\n",
        "concat.append(rem_input_dense)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwBwtl_aL3wn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "65d0d591-6586-4a04-ff16-401a41be5018"
      },
      "source": [
        "#After concatenating text input, categorical and remaining numerical features, applying it to the model\n",
        "from keras.layers import Concatenate\n",
        "x = Concatenate()(concat)\n",
        "#X=BatchNormalization()(X)\n",
        "x= Dense(256,kernel_initializer='glorot_normal',kernel_regularizer=l2(0.002))(x)\n",
        "#x= LeakyReLU(alpha = 0.3)(x)\n",
        "x= Dropout(0.6)(x)\n",
        "x= Dense(128,kernel_initializer='glorot_normal',kernel_regularizer=l2(0.002))(x)\n",
        "#x= LeakyReLU(alpha = 0.3)(x)\n",
        "x= Dropout(0.5)(x)\n",
        "x= Dense(64,kernel_initializer='glorot_normal',kernel_regularizer=l2(0.002))(x)\n",
        "#x= LeakyReLU(alpha = 0.3)(x)\n",
        "x= Dropout(0.5)(x)\n",
        "x= Dense(32,kernel_initializer='glorot_normal',kernel_regularizer=l2(0.002))(x)\n",
        "#x= LeakyReLU(alpha = 0.3)(x)\n",
        "x= Dropout(0.5)(x)\n",
        "#x=BatchNormalization()(x)\n",
        "x= Dense(16,activation='relu',kernel_initializer='glorot_normal',kernel_regularizer=l2(0.002))(x)\n",
        "#x= LeakyReLU(alpha = 0.3)(x)\n",
        "#x= Dropout(0.25)(x)\n",
        "output=Dense(2, activation='softmax')(x)\n",
        "model_l = Model(inputs=ins, outputs=output)\n",
        "model_l.summary()\n",
        "#,kernel_initializer='glorot_normal',kernel_regularizer=l2(0.002)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "text_input (InputLayer)         (None, 400)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 400, 300)     14192100    text_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "teacher_prefix (InputLayer)     (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "school_state (InputLayer)       (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "project_grade_category (InputLa (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "clean_categories (InputLayer)   (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "clean_subcategories (InputLayer (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 400, 128)     219648      embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 1, 3)         18          teacher_prefix[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 1, 26)        1352        school_state[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 1, 3)         15          project_grade_category[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, 1, 26)        1326        clean_categories[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 1, 50)        19550       clean_subcategories[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "rem_input_layer (InputLayer)    (None, 3)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 51200)        0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 3)            0           embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 26)           0           embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 3)            0           embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 26)           0           embedding_5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_6 (Flatten)             (None, 50)           0           embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 64)           256         rem_input_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 51372)        0           flatten_1[0][0]                  \n",
            "                                                                 flatten_2[0][0]                  \n",
            "                                                                 flatten_3[0][0]                  \n",
            "                                                                 flatten_4[0][0]                  \n",
            "                                                                 flatten_5[0][0]                  \n",
            "                                                                 flatten_6[0][0]                  \n",
            "                                                                 dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 256)          13151488    concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 256)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 128)          0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 64)           8256        dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 64)           0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 32)           2080        dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 32)           0           dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 16)           528         dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 2)            34          dense_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 27,629,547\n",
            "Trainable params: 13,437,447\n",
            "Non-trainable params: 14,192,100\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byhAgHbmOV25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ins."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXBUTh3VL8--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def auroc(y_true, y_pred):\n",
        "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnqJMe1IL_eE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "ec901202-b249-489f-8f9f-6be6364ef857"
      },
      "source": [
        "import keras\n",
        "adam = keras.optimizers.Adam(lr=0.001)\n",
        "model_l.compile(optimizer=adam, loss='categorical_crossentropy',metrics=[auroc])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-25-4a25250c5bd7>:5: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUNhwnVrMCze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import *\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', patience=10,verbose=1)\n",
        "\n",
        "batch_size = 512\n",
        "filepath = '/content/drive/My Drive/Applied ML assignments/Epoch/epochs:{epoch:03d}-val_auc:{val_auroc:.3f}.hdf5'\n",
        "#earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
        "mcp_save = ModelCheckpoint(filepath, save_best_only=True, monitor='val_auc', mode='max')\n",
        "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, verbose=1,min_lr=0.001, mode='min')\n",
        "callbacks=[es, mcp_save, reduce_lr_loss]\n",
        "#model.fit(Xtr_more, Ytr_more, batch_size=batch_size, epochs=50, verbose=0, callbacks=[earlyStopping, mcp_save, reduce_lr_loss], validation_split=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOldBI5dat3J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 919
        },
        "outputId": "41a401dd-4fb4-47e2-ca89-d22f03f053c2"
      },
      "source": [
        "history_1= model_l.fit({'text_input': essay_pad_train, 'school_state': schoolstate_one_hot_train, 'project_grade_category': project_grade_category_one_hot_train,'clean_categories': categories_one_hot_train,'clean_subcategories':subcategories_one_hot_train, 'teacher_prefix':teacherprefix_ohe_train, 'rem_input_layer':rem_input_train},\n",
        "          y_train,epochs=20, batch_size=512,verbose=1, \n",
        "          validation_data=({'text_input': essay_pad_cv, 'school_state': schoolstate_one_hot_cv, 'project_grade_category': project_grade_category_one_hot_cv,'clean_categories': categories_one_hot_cv,'clean_subcategories':subcategories_one_hot_cv, 'teacher_prefix':teacherprefix_ohe_cv, 'rem_input_layer':rem_input_cv},\n",
        "          y_cv),callbacks=callbacks)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 69918 samples, validate on 17480 samples\n",
            "Epoch 1/20\n",
            "69918/69918 [==============================] - 1093s 16ms/step - loss: 3.6809 - auroc: 0.4961 - val_loss: 3.1844 - val_auroc: 0.5000\n",
            "Epoch 2/20\n",
            "69918/69918 [==============================] - 1079s 15ms/step - loss: 3.0693 - auroc: 0.4969 - val_loss: 2.9966 - val_auroc: 0.4979\n",
            "Epoch 3/20\n",
            "69918/69918 [==============================] - 1081s 15ms/step - loss: 2.9487 - auroc: 0.4902 - val_loss: 2.9057 - val_auroc: 0.4895\n",
            "Epoch 4/20\n",
            "69918/69918 [==============================] - 1104s 16ms/step - loss: 2.8859 - auroc: 0.4809 - val_loss: 2.8666 - val_auroc: 0.4806\n",
            "Epoch 5/20\n",
            "69918/69918 [==============================] - 1080s 15ms/step - loss: 2.5507 - auroc: 0.4643 - val_loss: 0.9984 - val_auroc: 0.4263\n",
            "Epoch 6/20\n",
            "69918/69918 [==============================] - 1076s 15ms/step - loss: 0.8215 - auroc: 0.4935 - val_loss: 0.7143 - val_auroc: 0.5951\n",
            "Epoch 7/20\n",
            "69918/69918 [==============================] - 1144s 16ms/step - loss: 0.7151 - auroc: 0.5242 - val_loss: 0.6793 - val_auroc: 0.5762\n",
            "Epoch 8/20\n",
            "69918/69918 [==============================] - 1088s 16ms/step - loss: 0.6664 - auroc: 0.5741 - val_loss: 0.6411 - val_auroc: 0.6230\n",
            "Epoch 9/20\n",
            "69918/69918 [==============================] - 1089s 16ms/step - loss: 0.6234 - auroc: 0.6225 - val_loss: 0.6082 - val_auroc: 0.6855\n",
            "Epoch 10/20\n",
            "69918/69918 [==============================] - 1099s 16ms/step - loss: 0.5835 - auroc: 0.6848 - val_loss: 0.5623 - val_auroc: 0.7130\n",
            "Epoch 11/20\n",
            "69918/69918 [==============================] - 1096s 16ms/step - loss: 0.6467 - auroc: 0.6089 - val_loss: 0.5537 - val_auroc: 0.7034\n",
            "Epoch 12/20\n",
            "69918/69918 [==============================] - 1097s 16ms/step - loss: 0.5459 - auroc: 0.6901 - val_loss: 0.5470 - val_auroc: 0.7062\n",
            "Epoch 13/20\n",
            "69918/69918 [==============================] - 1050s 15ms/step - loss: 0.5167 - auroc: 0.7101 - val_loss: 0.5164 - val_auroc: 0.7162\n",
            "Epoch 14/20\n",
            "69918/69918 [==============================] - 1053s 15ms/step - loss: 0.4984 - auroc: 0.7155 - val_loss: 0.4858 - val_auroc: 0.7256\n",
            "Epoch 15/20\n",
            "69918/69918 [==============================] - 1086s 16ms/step - loss: 0.4794 - auroc: 0.7208 - val_loss: 0.4680 - val_auroc: 0.7249\n",
            "Epoch 16/20\n",
            "69918/69918 [==============================] - 1104s 16ms/step - loss: 0.4624 - auroc: 0.7269 - val_loss: 0.4526 - val_auroc: 0.7300\n",
            "Epoch 17/20\n",
            "69918/69918 [==============================] - 1097s 16ms/step - loss: 0.4469 - auroc: 0.7313 - val_loss: 0.4433 - val_auroc: 0.7290\n",
            "Epoch 18/20\n",
            "69918/69918 [==============================] - 1099s 16ms/step - loss: 0.4335 - auroc: 0.7365 - val_loss: 0.4262 - val_auroc: 0.7357\n",
            "Epoch 19/20\n",
            "69918/69918 [==============================] - 1096s 16ms/step - loss: 0.4238 - auroc: 0.7423 - val_loss: 0.4182 - val_auroc: 0.7385\n",
            "Epoch 20/20\n",
            "69918/69918 [==============================] - 1186s 17ms/step - loss: 0.4177 - auroc: 0.7425 - val_loss: 0.4156 - val_auroc: 0.7374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E9rvM0cMIuA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "74e2bc74-c321-4613-afff-f1ab2da4f288"
      },
      "source": [
        "custom_objects = {\"auroc\":auroc}\n",
        "#from keras.models import load_model\n",
        "#best_model_2 = load_model('/content/drive/My Drive/Applied ML assignments/Epoch/epochs:011-val_acc:0.673.hdf5',custom_objects=custom_objects)\n",
        "result = model_l.evaluate({'text_input': essay_pad_test, 'school_state': schoolstate_one_hot_test, 'project_grade_category': project_grade_category_one_hot_test,'clean_categories': categories_one_hot_test,'clean_subcategories':subcategories_one_hot_test, 'teacher_prefix':teacherprefix_ohe_test, 'rem_input_layer':rem_input_test},\n",
        "          y_test,batch_size=512)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21850/21850 [==============================] - 119s 5ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-cjopYCjMhC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1c158f14-54e6-478e-8c1e-f2cad702616a"
      },
      "source": [
        "print(\"{} of test data {}\". format(model_l.metrics_names[0],result[0]))\n",
        "print(\"{} of test data {}\". format(model_l.metrics_names[1],result[1]))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss of test data 0.4113458995306246\n",
            "auroc of test data 0.7382636455252072\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}